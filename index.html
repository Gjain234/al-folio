<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Gauri Jain</title> <meta name="author" content="Gauri Jain"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://gjain234.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Gauri</span> Jain </h1> <p class="desc">Advised by <a href="https://elisabethpaulson.github.io/" rel="external nofollow noopener" target="_blank">Elisabeth Paulson</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>Hello! My name is <a href="https://youtu.be/DZ6DOqpxTjg?t=8" rel="external nofollow noopener" target="_blank">Gauri</a> and I’m a third year Computer Science PhD student at Harvard. I am excited about all things related to using technology (carefully!!) to make the world better. For my research, I’m especially interested in applying concepts in optimization and machine learning to problems in public health and immigration.</p> <p>Before Harvard, I worked at Meta as a software engineer on Marketplace and Integrity products in the Bay Area and London. And before that, I studied computer science at Cornell University.</p> <p>I also love teaching (probably unsurprising given how overly energetic this page is)! Some of my favorite teaching experiences I’ve had so far are as a middle school math and cs instructor with <a href="https://www.breakthroughcollaborative.org/teach/" rel="external nofollow noopener" target="_blank">Breakthrough</a>, a Physics TA with the <a href="https://cpep.cornell.edu/" rel="external nofollow noopener" target="_blank">Cornell Prison Education Program</a>, and a Section Leader with the <a href="https://codeinplace.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford Code in Place Program</a>. If any of my background interests you, please reach out! I could probably learn a thing or two from you.</p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Nov 24, 2024</th> <td> I presented our work at PRICAI in Kyoto and our team received the <strong>Runner-up Best Paper</strong> award!. </td> </tr> <tr> <th scope="row">Oct 8, 2024</th> <td> I spoke to <a href="https://www.jessiefin.com/" rel="external nofollow noopener" target="_blank">Professor Finocchiaro’s</a> Algorithmic Fairness class at Boston College!. </td> </tr> <tr> <th scope="row">Feb 1, 2024</th> <td> I began working with Elisabeth and the <a href="https://immigrationlab.org/geomatch/" rel="external nofollow noopener" target="_blank">Geomatch Team</a>. </td> </tr> <tr> <th scope="row">Jun 1, 2023</th> <td> I spent the summer in India working as an intern for <a href="https://armman.org/" rel="external nofollow noopener" target="_blank">Armman</a>. </td> </tr> <tr> <th scope="row">Jan 7, 2023</th> <td> I visited the <a href="https://armman.org/" rel="external nofollow noopener" target="_blank">Armman</a> offices in India to learn more about how they are using telehealth programming to improve maternal and child health in India. </td> </tr> </table> </div> </div> <div class="publications"> <h2>research</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">PRICAI 2024</abbr></div> <div id="irlpricai" class="col-sm-8"> <div class="title">IRL for Restless Multi-armed Bandits with Applications in Maternal and Child Health</div> <div class="author"> Gauri Jain, Pradeep Varakantham, Haifeng Xu, Aparna Taneja, Prashant Doshi, and Milind Tambe</div> <div class="periodical"> <em>PRICAI</em> Nov 2024 </div> <p style="font-weight: bold; color: var(--global-theme-color);">⭐ Best Paper Runner-Up ⭐</p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/chapter/10.1007/978-981-96-0128-8_15" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="abstract hidden"> <p>Public health practitioners often have the goal of monitoring patients and maximizing patients’ time spent in “favorable" or healthy states while being constrained to using limited resources. Restless multi-armed bandits (RMAB) are an effective model to solve this problem as they are helpful to allocate limited resources among many agents under resource constraints, where patients behave differently depending on whether they are intervened on or not. However, RMABs assume the reward function is known. This is unrealistic in many public health settings because patients face unique challenges and it is impossible for a human to know who is most deserving of any intervention at such a large scale. To address this shortcoming, this paper is the first to present the use of inverse reinforcement learning (IRL) to learn desired rewards for RMABs, and we demonstrate improved outcomes in a maternal and child health telehealth program. First we allow public health experts to specify their goals at an aggregate or population level and propose an algorithm to design expert trajectories at scale based on those goals. Second, our algorithm WHIRL uses gradient updates to optimize the objective, allowing for efficient and accurate learning of RMAB rewards. Third, we compare with existing baselines and outperform those in terms of run-time and accuracy. Finally, we evaluate and show the usefulness of WHIRL on thousands on beneficiaries from a real-world maternal and child health setting in India. We publicly release our code here: https://github.com/Gjain234/WHIRL.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IJCAI 2023 Workshop</abbr></div> <div id="PhysRev.47.777" class="col-sm-8"> <div class="title">Inverse Reinforcement Learning for Restless Multi-Armed Bandits with Application to Maternal and Child Health</div> <div class="author"> Gauri Jain, Pradeep Varakantham, Haifeng Xu, Aparna Taneja, and Milind Tambe</div> <div class="periodical"> <em>IJCAI Bridge AI Workshop</em> Aug 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://sites.google.com/view/bridgeai/accepted-papers" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="abstract hidden"> <p>We study restless multi-armed bandits (RMABs) in the context of public health, where there is a need to optimize resource allocation decisions. Until now, RMABs typically solve for the optimal planning policy by assuming the reward function in the problem is fully known. However, in this work, we aim to study whether we can learn the most optimal rewards for an RMAB problem given some demonstrated, ideal behavior. To achieve this, we turn to inverse reinforcement learning (IRL) which is a field of study motivated by the desire to understand and learn the underlying reward structure of an agent’s observed behavior. Existing IRL approaches predominantly focus on single agent systems, presenting limitations in dealing with the expansive state spaces characteristic of public health scenarios, where tens of thousands of arms are active simultaneously. We propose a new IRL algorithm specifically for RMAB settings that uses techniques from decision focused learning (DFL) to directly optimize the objective function, allowing for efficient and accurate updates to the learned rewards. We compare our algorithm with the max entropy IRL baseline on runtime and accuracy and find that our algorithm performs better on both metrics. We also propose a framework for how to apply this algorithm in the public health domain where expert trajectories come from domain experts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">AAAI 2023 Workshop</abbr></div> <div id="PhysRev.47.778" class="col-sm-8"> <div class="title">BiomeAzuero2022: A Fine-Grained Dataset and Baselines for Tree Species Classification with Ground Images</div> <div class="author"> Ziwei Gu*, Gauri Jain*, Hongwen Song*, Isak Diaz Diaz, Margaux Masson-Forsythe, and Jorge Valdes</div> <div class="periodical"> <em>AAAI AI for Social Good Workshop</em> Feb 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://amulyayadav.github.io/AI4SG2023/images/15.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="abstract hidden"> <p>It is becoming increasingly popular for organizations to invest in carbon credits programs involving planting trees to offset their carbon footprint. However, in order to correctly measure carbon sequestration, it is important to know which tree species exist in a given region. This identification task requires very well-trained local botanists who are not only an expensive resource, but also not accurate enough. AIassisted tree classification has become an promising way to solve the problem, but low data quality has been the limiting factor. Our work focuses on determining how to build a high quality dataset of tree parts that can lead to the most accurate tree classification. We contribute BiomeAzuero2022, a publicly available image dataset of 9071 tree images captured in Azuero, Panama with ground truth species labels, organized by different parts of trees. We further provide a methodology for estimating feature importance via machine learning and interpretability methods with the goal of gathering higher-quality image data through a case study on BiomeAzuero2022.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Informs 2022</abbr></div> <div id="PhysRev.47.780" class="col-sm-8"> <div class="title">Sequential Fair Allocation: Achieving the Optimal Envy-Efficiency Tradeoff Curve</div> <div class="author"> Sean Sinclair, Gauri Jain, Siddhartha Banerjee, and Christina Lee Yu</div> <div class="periodical"> <em>Operations Research</em> Nov 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2105.05308" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="abstract hidden"> <p>We consider the problem of dividing limited resources to individuals arriving over T rounds. Each round has a random number of individuals arrive, and individuals can be characterized by their type (i.e. preferences over the different resources). A standard notion of ’fairness’ in this setting is that an allocation simultaneously satisfy envy-freeness and efficiency. The former is an individual guarantee, requiring that each agent prefers their own allocation over the allocation of any other; in contrast, efficiency is a global property, requiring that the allocations clear the available resources. For divisible resources, when the number of individuals of each type are known upfront, the above desiderata are simultaneously achievable for a large class of utility functions. However, in an online setting when the number of individuals of each type are only revealed round by round, no policy can guarantee these desiderata simultaneously, and hence the best one can do is to try and allocate so as to approximately satisfy the two properties. We show that in the online setting, the two desired properties (envy-freeness and efficiency) are in direct contention, in that any algorithm achieving additive counterfactual envy-freeness up to a factor of LT necessarily suffers a efficiency loss of at least 1/LT. We complement this uncertainty principle with a simple algorithm, HopeGuardrail, which allocates resources based on an adaptive threshold policy and is able to achieve any fairness-efficiency point on this frontier. In simulation results, our algorithm provides allocations close to the optimal fair solution in hindsight, motivating its use in practical applications as the algorithm is able to adapt to any desired fairness efficiency trade-off.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS 2020</abbr></div> <div id="PhysRev.47.781" class="col-sm-8"> <div class="title">Adaptive Discretization for Model-Based Reinforcement Learning</div> <div class="author"> Sean Sinclair, Tianyu Wang, Gauri Jain, Siddhartha Banerjee, and Christina Lee Yu</div> <div class="periodical"> <em>Advances in Neural Information Processing Systems 33</em> May 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper/2020/file/285baacbdf8fda1de94b19282acd23e2-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="abstract hidden"> <p>We introduce the technique of adaptive discretization to design an efficient model-based episodic reinforcement learning algorithm in large (potentially continuous) state-action spaces. Our algorithm is based on optimistic one-step value iteration extended to maintain an adaptive discretization of the space. From a theoretical perspective we provide worst-case regret bounds for our algorithm which are competitive compared to the state-of-the-art model-based algorithms. Moreover, our bounds are obtained via a modular proof technique which can potentially extend to incorporate additional structure on the problem. From an implementation standpoint, our algorithm has much lower storage and computational requirements due to maintaining a more efficient partition of the state and action spaces. We illustrate this via experiments on several canonical control problems, which shows that our algorithm empirically performs significantly better than fixed discretization in terms of both faster convergence and lower memory usage. Interestingly, we observe empirically that while fixed discretization model-based algorithms vastly outperform their model-free counterparts, the two achieve comparable performance with adaptive discretization.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%67%61%75%72%69%6A%61%69%6E@%67.%68%61%72%76%61%72%64.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://gjain234.github.io/assets/pdf/Gauri_Resume.pdf" title="cv">cv</a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Gauri Jain. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>